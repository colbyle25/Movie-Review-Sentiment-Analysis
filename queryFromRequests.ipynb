{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping IMDB User Reviews\n",
    "- To answer how Americans felt about Barbie vs. Oppenhiemer, we decided the first step was to scrape and collect user reviews from the popular movie rating website IMDB.\n",
    "\n",
    "### In this build:\n",
    "- Requirements:\n",
    "    - Selenium `pip install selenium`\n",
    "    - Pandas `pip install pandas`\n",
    "- Links to User Reviews\n",
    "    - [Oppenhiemer](https://www.imdb.com/title/tt15398776/reviews)\n",
    "    - [Barbie](https://www.imdb.com/title/tt1517268/reviews)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By  # Import By\n",
    "\n",
    "PATH = r\"/Users/Maajid/Downloads/chromedriver-mac-x64/chromedriver\"\n",
    "def scrape_reviews(url):\n",
    "    # Setup WebDriver (Ensure you have the correct path to your WebDriver)\n",
    "    service = Service(PATH)  # You need to pass the PATH to your Service\n",
    "    options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Initialize lists to store review data\n",
    "    titles = []\n",
    "    contents = []\n",
    "    ratings = []\n",
    "    dates = []\n",
    "\n",
    "    # Wait for the page to load\n",
    "    driver.implicitly_wait(5)\n",
    "    page = 1\n",
    "\n",
    "    # We want at least 1000 review, so get 50 at a safe number\n",
    "    while page < 50:  \n",
    "        try:\n",
    "            # Find the load more button on the webpage\n",
    "            load_more = driver.find_element(By.ID, 'load-more-trigger')  # Update to use By.ID\n",
    "            # Click on that button\n",
    "            load_more.click()\n",
    "            page += 1\n",
    "        except:\n",
    "            # If couldn't find any more button to click, stop\n",
    "            break\n",
    "\n",
    "    # Find and iterate over each review\n",
    "    review = driver.find_elements(By.CLASS_NAME, 'review-container')  # Update to use By.CLASS_NAME\n",
    "    # Set list for each element:\n",
    "    review_x =[]\n",
    "    title = []\n",
    "    content = []\n",
    "    rating = []\n",
    "    date = []\n",
    "    user_name = []\n",
    "    # Run for loop to get \n",
    "    for n in range(0, 1100):\n",
    "        try:\n",
    "            # Some reviewers only give review text or rating without the other, \n",
    "            # so we use try/except here to make sure each block of content must has all the elements before append them to the list\n",
    "\n",
    "            # Check if each review has all the elements\n",
    "            review_x_tmp = review[n].text\n",
    "            # ftitle = review[n].find_element(By.CLASS_NAME, 'title').text\n",
    "            # fcontent = review[n].find_element(By.CLASS_NAME, 'content').get_attribute(\"textContent\").strip()\n",
    "            # frating = review[n].find_element(By.CLASS_NAME, 'rating-other-user-rating').text\n",
    "            # fdate = review[n].find_element(By.CLASS_NAME, 'review-date').text\n",
    "            # fname = review[n].find_element(By.CLASS_NAME, 'display-name-link').text\n",
    "\n",
    "            # Then add them to the respective list\n",
    "            review_x.append(review_x_tmp)\n",
    "            # title.append(ftitle)\n",
    "            # content.append(fcontent)\n",
    "            # rating.append(frating)\n",
    "            # date.append(fdate)\n",
    "            # user_name.append(fname)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Create a DataFrame from the scraped data\n",
    "    # reviews_df = pd.DataFrame({\n",
    "    #     'Title': titles,\n",
    "    #     'Content': contents,\n",
    "    #     'Rating': ratings,\n",
    "    #     'Date': dates,\n",
    "    # })\n",
    "\n",
    "    return review_x\n",
    "\n",
    "# URLs for \"Barbie\" and \"Oppenheimer\" reviews\n",
    "barbie_reviews_url = 'https://www.imdb.com/title/tt15398776/reviews'\n",
    "oppenheimer_reviews_url = 'https://www.imdb.com/title/tt1517268/reviews'\n",
    "\n",
    "# Scrape reviews for each movie and save to CSV\n",
    "oppenheimer_reviews = scrape_reviews(barbie_reviews_url)\n",
    "barbie_reviews = scrape_reviews(oppenheimer_reviews_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date, and review text from oppenheimer_reviews and barbie_reviews\n",
    "def create_df(reviews):\n",
    "    reviews_df = pd.DataFrame(reviews, columns=['review'])\n",
    "    reviews_df['score'] = reviews_df['review'].apply(lambda x: x.split('\\n')[0])\n",
    "    reviews_df['title'] = reviews_df['review'].apply(lambda x: x.split('\\n')[1])\n",
    "    reviews_df['date'] = reviews_df['review'].apply(lambda x: ' '.join(x.split('\\n')[2].split(' ')[-2:]))\n",
    "    reviews_df['text'] = reviews_df['review'].apply(lambda x: ' '.join(x.split('\\n')[3:4]))\n",
    "    reviews_df.drop(columns=['review'], inplace=True)\n",
    "    return reviews_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_date(reviews_df):\n",
    "    reviews_df['date'] = pd.to_datetime(reviews_df['date'], errors='coerce')\n",
    "    reviews_df['date'] = reviews_df['date'].dt.strftime('%m %Y')\n",
    "    reviews_df = reviews_df.dropna(subset=['date'])\n",
    "    reviews_df = reviews_df[(reviews_df['date'] >= '07 2023') & (reviews_df['date'] < '10 2023')]\n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "oppenheimer_reviews_df = create_df(oppenheimer_reviews)\n",
    "barbie_reviews_df = create_df(barbie_reviews)\n",
    "\n",
    "oppenheimer_reviews_df = filter_date(oppenheimer_reviews_df)\n",
    "barbie_reviews_df = filter_date(barbie_reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "oppenheimer_reviews_df.to_csv('oppenheimer_reviews.csv', index=False)\n",
    "barbie_reviews_df.to_csv('barbie_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Transformer for Setiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE for visualizing clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
